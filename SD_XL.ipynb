{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWBnrpDVDPbn"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate safetensors --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to image"
      ],
      "metadata": {
        "id": "imjvd4jpDxiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"cagliostrolab/animagine-xl-3.1\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "pipe.to('cuda')\n",
        "\n"
      ],
      "metadata": {
        "id": "2wDCAnPTLL0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output"
      ],
      "metadata": {
        "id": "dVvKR3xrMdmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen1 = int(input())\n"
      ],
      "metadata": {
        "id": "TuUCPAj59teJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import subprocess\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "#prompt = \"1girl, souryuu asuka langley, neon genesis evangelion, solo, upper body, v, smile, looking at viewer, outdoors, night\"\n",
        "prompt_1 = \"1girl, Clementine, overlord, solo, upper body,\"\n",
        "prompt_2 =  \",looking at viewer, outdoors, night,\"\n",
        "#negative_prompt = \"nsfw, lowres, (bad), text, error, fewer, extra, missing, worst quality, jpeg artifacts, low quality, watermark, unfinished, displeasing, oldest, early, chromatic aberration, signature, extra digits, artistic error, username, scan, [abstract]\"\n",
        "negative_prompt = \"lowres, (bad), text, error, fewer, extra, missing, worst quality, jpeg artifacts, low quality, watermark, unfinished, displeasing, oldest, early, chromatic aberration, signature, extra digits, artistic error, username, scan, [abstract]\"\n",
        "\n",
        "expressions = ['smile','sad','angry','shy']\n",
        "if not gen1 :\n",
        "  gen1 = random.randint(0,999999999)\n",
        "for i in expressions :\n",
        "  image = pipe(\n",
        "    prompt_1+i+prompt_2,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=1920,\n",
        "    height=1080,\n",
        "    guidance_scale=7,\n",
        "  generator=torch.manual_seed(gen1),\n",
        "    num_inference_steps=28\n",
        "  ).images\n",
        "  print(\"Length of images is :\",len(image))\n",
        "  DATETIME= subprocess.check_output([\"date\",\"+%s\"]).decode(\"utf-8\")\n",
        "  image[0].save(\"./output/\"+DATETIME+\".png\")\n",
        "image_array = []\n",
        "for i in os.listdir('./output') :\n",
        "  if \".png\" in i :\n",
        "    image_array.append(load_image(os.path.join('./output',i)))\n",
        "\n",
        "make_image_grid(image_array,rows=len(image_array),cols=1)\n"
      ],
      "metadata": {
        "id": "lg5PZIdyLYTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Modifications"
      ],
      "metadata": {
        "id": "C5O7ApB7S7Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from diffusers import AutoPipelineForImage2Image\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "\n",
        "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
        "pipeline = AutoPipelineForImage2Image.from_pipe(pipe).to(\"cuda\")"
      ],
      "metadata": {
        "id": "hSxXSDy7SuNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url = \"./megumin_2.png\"\n",
        "init_image = load_image(url)\n",
        "prompt_2 = prompt+\",smiling expression\"\n",
        "image = pipeline(prompt_2, image=init_image, strength=0.8, guidance_scale=12).images[0]\n",
        "make_image_grid([init_image, image], rows=1, cols=2)"
      ],
      "metadata": {
        "id": "FOWqbVS3S5hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Default SDXL"
      ],
      "metadata": {
        "id": "cGOmkm5KLiEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WLl2VBFwDw0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
        "prompt = input(\"Enter your prompt\")\n",
        "image = pipeline_text2image(prompt=prompt).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "UtVsFY4wJPyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image to image"
      ],
      "metadata": {
        "id": "tZZlvf2uD2kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "from diffusers import AutoPipelineForImage2Image\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "\n",
        "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
        "pipeline = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n",
        "\n"
      ],
      "metadata": {
        "id": "npFFrp4nD7oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"./inputs/megumin_2.png\"\n",
        "init_image = load_image(url)\n",
        "prompt = \"smiling expression\"\n",
        "image = pipeline(prompt, image=init_image, strength=0.8, guidance_scale=10.5).images[0]\n",
        "make_image_grid([init_image, image], rows=1, cols=2)"
      ],
      "metadata": {
        "id": "WLHAqRuvRCf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inpainting"
      ],
      "metadata": {
        "id": "xMqPetnZD-1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "from diffusers import AutoPipelineForInpainting\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "\n",
        "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
        "pipeline = AutoPipelineForInpainting.from_pipe(pipeline_text2image).to(\"cuda\")\n",
        "\n",
        "img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n",
        "mask_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-inpaint-mask.png\"\n",
        "\n",
        "init_image = load_image(img_url)\n",
        "mask_image = load_image(mask_url)\n",
        "\n",
        "prompt = \"A deep sea diver floating\"\n",
        "image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, strength=0.85, guidance_scale=12.5).images[0]\n",
        "make_image_grid([init_image, mask_image, image], rows=1, cols=3)"
      ],
      "metadata": {
        "id": "fcUFqTFrEB2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base + Refiner"
      ],
      "metadata": {
        "id": "UJ1iF8G6EMaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "base = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "refiner = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "    text_encoder_2=base.text_encoder_2,\n",
        "    vae=base.vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "8BpItF6OEPAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A majestic lion jumping from a big stone at night\"\n",
        "\n",
        "image = base(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=40,\n",
        "    denoising_end=0.8,\n",
        "    output_type=\"latent\",\n",
        ").images\n",
        "image = refiner(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=40,\n",
        "    denoising_start=0.8,\n",
        "    image=image,\n",
        ").images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "DsYAm3WwERKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The refiner model can also be used for inpainting in the StableDiffusionXLInpaintPipeline:"
      ],
      "metadata": {
        "id": "9CwkIxhxEVSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionXLInpaintPipeline\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "import torch\n",
        "\n",
        "base = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "refiner = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "    text_encoder_2=base.text_encoder_2,\n",
        "    vae=base.vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
        "\n",
        "init_image = load_image(img_url)\n",
        "mask_image = load_image(mask_url)\n",
        "\n",
        "prompt = \"A majestic tiger sitting on a bench\"\n",
        "num_inference_steps = 75\n",
        "high_noise_frac = 0.7\n",
        "\n",
        "image = base(\n",
        "    prompt=prompt,\n",
        "    image=init_image,\n",
        "    mask_image=mask_image,\n",
        "    num_inference_steps=num_inference_steps,\n",
        "    denoising_end=high_noise_frac,\n",
        "    output_type=\"latent\",\n",
        ").images\n",
        "image = refiner(\n",
        "    prompt=prompt,\n",
        "    image=image,\n",
        "    mask_image=mask_image,\n",
        "    num_inference_steps=num_inference_steps,\n",
        "    denoising_start=high_noise_frac,\n",
        ").images[0]\n",
        "make_image_grid([init_image, mask_image, image.resize((512, 512))], rows=1, cols=3)"
      ],
      "metadata": {
        "id": "HznD1O1pEb4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base to refiner model\n",
        "\n",
        "SDXL gets a boost in image quality by using the refiner model to add additional high-quality details to the fully-denoised image from the base model, in an image-to-image setting."
      ],
      "metadata": {
        "id": "JWissrQKEjy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "base = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "refiner = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "    text_encoder_2=base.text_encoder_2,\n",
        "    vae=base.vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "5O7y9kr2EouC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
        "\n",
        "image = base(prompt=prompt, output_type=\"latent\").images[0]"
      ],
      "metadata": {
        "id": "NPOBfs8gErYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use a different prompt for each text-encoder\n"
      ],
      "metadata": {
        "id": "gH2RujE1E_R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "# prompt is passed to OAI CLIP-ViT/L-14\n",
        "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
        "# prompt_2 is passed to OpenCLIP-ViT/bigG-14\n",
        "prompt_2 = \"Van Gogh painting\"\n",
        "image = pipeline(prompt=prompt, prompt_2=prompt_2).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "pUTARiH5FA2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizations\n",
        "\n",
        "\n",
        "SDXL is a large model, and you may need to optimize memory to get it to run on your hardware. Here are some tips to save memory and speed up inference.\n",
        "\n",
        "1. Offload the model to the CPU with enable_model_cpu_offload() for out-of-memory errors:\n",
        "\n",
        "```\n",
        "- base.to(\"cuda\")\n",
        "- refiner.to(\"cuda\")\n",
        "+ base.enable_model_cpu_offload()\n",
        "+ refiner.enable_model_cpu_offload()\n",
        "```\n",
        "\n",
        "2. Use torch.compile for ~20% speed-up (you need torch>=2.0):\n",
        "\n",
        "```\n",
        "+ base.unet = torch.compile(base.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "+ refiner.unet = torch.compile(refiner.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "```\n",
        "\n",
        "3. Enable xFormers to run SDXL if torch<2.0:\n",
        "\n",
        "```\n",
        "+ base.enable_xformers_memory_efficient_attention()\n",
        "+ refiner.enable_xformers_memory_efficient_attention()\n",
        "```\n"
      ],
      "metadata": {
        "id": "1ZQe4QnNFEIz"
      }
    }
  ]
}